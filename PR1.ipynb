{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from nltk.stem import PorterStemmer \n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of preprocessing functions\n",
    "\n",
    "# Functional functions\n",
    "def compose2(f, g):\n",
    "    return lambda *a, **kw: f(g(*a, **kw))\n",
    "\n",
    "def compose (*functions):\n",
    "    def inner(arg):\n",
    "        for f in reversed(functions):\n",
    "            arg = f(arg)\n",
    "        return arg\n",
    "    return inner\n",
    "\n",
    "# Filtering\n",
    "has_valid_len = lambda w: len(w) >= 3\n",
    "is_nonstop_word = lambda w: w not in ENGLISH_STOP_WORDS\n",
    "is_valid_word = lambda w: has_valid_len(w) and is_nonstop_word(w)\n",
    "\n",
    "# Mapping\n",
    "to_lower = lambda w: w.lower()\n",
    "remove_punc = lambda w: w.translate(str.maketrans('', '', string.punctuation))\n",
    "remove_num = lambda w: re.sub(r'\\d+', '', w)\n",
    "\n",
    "map_word = compose(to_lower, remove_punc, remove_num)\n",
    "\n",
    "# This mapping is expensive, so do it after filtering\n",
    "ps = PorterStemmer() \n",
    "stem = lambda w: ps.stem(w)\n",
    "\n",
    "def preprocess(docs):\n",
    "    docs = [ [map_word(t) for t in d ] for d in docs ]\n",
    "    docs = [ [t for t in d if is_valid_word(t)] for d in docs ]\n",
    "    docs = [ [stem(t) for t in d ] for d in docs ]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse matrices, which requires aggregration of term IDs for both training and test data.\n",
    "\n",
    "def calc_term_ids(docs):\n",
    "    r\"\"\" The docs should be the combination of training set and test set.\"\"\"\n",
    "    term_ids = {}\n",
    "    curr_term_id = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in term_ids:\n",
    "                term_ids[w] = curr_term_id\n",
    "                curr_term_id += 1\n",
    "    return (term_ids, nnz)\n",
    "\n",
    "def build_sparse_matrix(docs, term_ids = {}, nnz = 0):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    ncols = len(term_ids)\n",
    "    assert(ncols != 0)\n",
    "\n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows + 1, dtype=np.int)\n",
    "    row_id = 0  # document ID / row counter\n",
    "    acc = 0  # non-zero counter\n",
    "\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k, _ in cnt.most_common())\n",
    "        curr_doc_len = len(keys)\n",
    "        for i, key in enumerate(keys):\n",
    "            ind[acc + i] = term_ids[key]\n",
    "            val[acc + i] = cnt[key]\n",
    "        ptr[row_id + 1] = ptr[row_id] + curr_doc_len\n",
    "        acc += curr_doc_len\n",
    "        row_id += 1\n",
    "\n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale matrix and normalize its rows\n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf. \n",
    "    Returns scaling factors as dict. If copy is True, \n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else mat\n",
    "\n",
    "def csr_l2normalize(mat, copy=False):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n",
    "    \n",
    "def csr_normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" combination of 'csr_idf' and 'csr_l2normalize' \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    csr_idf(mat)\n",
    "    csr_l2normalize(mat)\n",
    "    if copy is True:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_w(test_data, train_data, labels, k=1):\n",
    "    r\"\"\" Predict the label of test data based on the given\n",
    "    labeled training data using k-nearest neighbor classifier.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    similarity = train_data.dot(test_data.T).todense()\n",
    "    top_k_idx = np.argpartition(similarity, -k, axis=0)[-k:,:]\n",
    "    to_labels = np.vectorize(lambda idx: labels[idx])\n",
    "    top_k_label = to_labels(top_k_idx)\n",
    "    \n",
    "    n_test_col = similarity.shape[1]\n",
    "\n",
    "    for col in range(n_test_col):\n",
    "        train_tags = top_k_label[:,col].flatten().tolist()[0]\n",
    "        train_indices = top_k_idx[:,col].flatten().tolist()[0]\n",
    "\n",
    "        # Select the maximum aggregated similarity\n",
    "        weights = defaultdict(float)\n",
    "        for tag, row in zip(train_tags, train_indices):\n",
    "            weights[tag] += similarity[row, col]\n",
    "        result = max(weights.items(), key=itemgetter(1))[0]\n",
    "        \n",
    "        \n",
    " \n",
    "        predictions.append(result)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_as_lines(path):\n",
    "    r\"\"\" Open text file by path and read all lines \"\"\"\n",
    "    with open(path, \"r\") as fh:\n",
    "        lines = fh.readlines()\n",
    "        \n",
    "    # transform docs into lists of words\n",
    "    raw_lines = [l.split() for l in lines]\n",
    "\n",
    "    return raw_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_data_and_label(raw_lines):\n",
    "    r\"\"\" Split training data and label from raw lines \"\"\"\n",
    "    y = list(map(lambda x: int(x[0]), raw_lines))\n",
    "    x = list(map(lambda x: x[1:], raw_lines))\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result_as_file(prediction, file_name=\"prediction.dat\"):\n",
    "    r\"\"\" Save the predicted result as a new file \"\"\"\n",
    "    file_content = \"\\n\".join(list(map(str, prediction)))\n",
    "    with open(file_name, \"w\") as fd:\n",
    "        fd.write(file_content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Read files\n",
    "    train_lines = load_data_as_lines(\"train.dat\")\n",
    "    x_test = load_data_as_lines(\"test.dat\")\n",
    "    \n",
    "    # Split \n",
    "    x_train, y_train = to_data_and_label(train_lines)\n",
    "\n",
    "    # Text preprocessing\n",
    "    print(\"Preprocessing documents...\")\n",
    "    docs_train = preprocess(x_train)\n",
    "    docs_test = preprocess(x_test)\n",
    "\n",
    "    # Build shared term ID dictionary for classification\n",
    "    print(\"Calculating terms...\")\n",
    "    term_ids, nnz = calc_term_ids([*docs_train, *docs_test])\n",
    "\n",
    "    # Build sparse matrices\n",
    "    print(\"Building sparse matrix...\")\n",
    "    mat_train = build_sparse_matrix(docs_train, term_ids, nnz)\n",
    "    mat_test = build_sparse_matrix(docs_test, term_ids, nnz)\n",
    "\n",
    "    # Normalize CSR matrices\n",
    "    print(\"Normalizing matrix...\")\n",
    "    mat_test = csr_normalize(mat_test, copy=True)\n",
    "    mat_train = csr_normalize(mat_train, copy=True)\n",
    "\n",
    "    # Train\n",
    "    print(\"Start training...\")\n",
    "    y_test = predict(mat_test, mat_train, y_train, k=6)\n",
    "    save_result_as_file(y_test)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(raw_list, folds):\n",
    "    fold_size = math.floor(len(raw_list) / folds)\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for i in range(folds):\n",
    "        if i == folds - 1:\n",
    "            train_set.append(raw_list[:i*fold_size])\n",
    "            test_set.append(raw_list[i*fold_size:])\n",
    "        else:\n",
    "            train_set.append(raw_list[:i*fold_size] + raw_list[(i+1)*fold_size:])\n",
    "            test_set.append(raw_list[i*fold_size:(i+1)*fold_size])\n",
    "    return train_set, test_set\n",
    "\n",
    "def test(folds = 10, max_k = 500):\n",
    "    r\"\"\" Testing with cross validation \"\"\"\n",
    "    \n",
    "    print(\"Start testing mode...\")\n",
    "    scores = []\n",
    "\n",
    "    # Read files\n",
    "    train_lines = load_data_as_lines(\"train.dat\")\n",
    "    train_lines_set, test_lines_set = partition(train_lines, folds)\n",
    "    \n",
    "    train_lines_set = list(map(to_data_and_label, train_lines_set))\n",
    "    test_lines_set = list(map(to_data_and_label, test_lines_set))\n",
    "    \n",
    "    for i in range(folds):\n",
    "        print(\"> Running fold %d\" % (i + 1))\n",
    "        \n",
    "        x_train, y_train = train_lines_set[i]\n",
    "        x_test, y_test = test_lines_set[i]\n",
    "\n",
    "        # Text preprocessing\n",
    "        print(\"Preprocessing documents...\")\n",
    "        docs_train = preprocess(x_train)\n",
    "        docs_test = preprocess(x_test)\n",
    "\n",
    "        # Build shared term ID dictionary for classification\n",
    "        print(\"Calculating terms...\")\n",
    "        term_ids, nnz = calc_term_ids([*docs_train, *docs_test])\n",
    "\n",
    "        # Build sparse matrices\n",
    "        print(\"Building sparse matrix...\")\n",
    "        mat_train = build_sparse_matrix(docs_train, term_ids, nnz)\n",
    "        mat_test = build_sparse_matrix(docs_test, term_ids, nnz)\n",
    "\n",
    "        # Normalize CSR matrices\n",
    "        print(\"Normalizing matrix...\")\n",
    "        mat_test = csr_normalize(mat_test, copy=True)\n",
    "        mat_train = csr_normalize(mat_train, copy=True)\n",
    "\n",
    "        # Prediction\n",
    "        for k in range(max_k):\n",
    "            y_hat = predict(mat_test, mat_train, y_train, k=k)\n",
    "            scores.append(f1_score(y_test, y_hat, average='macro'))\n",
    "            \n",
    "    avg_score = np.average(scores, axis=1)\n",
    "    res = [{\"k\": i + 1, \"score\": avg_score[i]} for i in range(len(avg_score))]\n",
    "\n",
    "    return sorted(res, key=lambda x: x[\"score\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing documents...\n",
      "Calculating terms...\n",
      "Building sparse matrix...\n",
      "Normalizing matrix...\n",
      "Start training...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
