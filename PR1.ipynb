{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import string\n",
    "import re\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer \n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open docs file and read its lines\n",
    "with open(\"train.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform docs into lists of words\n",
    "raw_lines = [l.split() for l in lines]\n",
    "labels = list(map(lambda x: int(x[0]), raw_lines))\n",
    "docs = list(map(lambda x: x[1:], raw_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add validation set\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, labels, test_size=0.01)\n",
    "docs = x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of preprocess functions\n",
    "\n",
    "# Functional functions\n",
    "def compose2(f, g):\n",
    "    return lambda *a, **kw: f(g(*a, **kw))\n",
    "\n",
    "def compose (*functions):\n",
    "    def inner(arg):\n",
    "        for f in reversed(functions):\n",
    "            arg = f(arg)\n",
    "        return arg\n",
    "    return inner\n",
    "\n",
    "# Filtering\n",
    "has_valid_len = lambda w: len(w) >= 4\n",
    "is_nonstop_word = lambda w: w not in ENGLISH_STOP_WORDS\n",
    "is_valid_word = lambda w: has_valid_len(w) and is_nonstop_word(w)\n",
    "\n",
    "# Mapping\n",
    "to_lower = lambda w: w.lower()\n",
    "remove_punc = lambda w: w.translate(str.maketrans('', '', string.punctuation))\n",
    "remove_num = lambda w: re.sub(r'\\d+', '', w)\n",
    "\n",
    "map_word = compose(to_lower, remove_punc, remove_num)\n",
    "\n",
    "# This mapping is expensive, so do it after filtering\n",
    "ps = PorterStemmer() \n",
    "stem = lambda w: ps.stem(w)\n",
    "\n",
    "def preprocess(docs):\n",
    "    docs = [ [map_word(t) for t in d ] for d in docs ]\n",
    "    docs = [ [t for t in d if is_valid_word(t)] for d in docs ]\n",
    "#     docs = [ [stem(t) for t in d ] for d in docs ]\n",
    "    return docs\n",
    "\n",
    "def preprocess2(docs):\n",
    "    docs = preprocess(docs)\n",
    "    return [ [stem(t) for t in d ] for d in docs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train = preprocess2(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test = preprocess2(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse matrices, which requires aggregration of term IDs for both training and test data.\n",
    "\n",
    "def calc_term_ids(docs):\n",
    "    r\"\"\" The docs should be the combination of training set and test set.\"\"\"\n",
    "    term_ids = {}\n",
    "    curr_term_id = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in term_ids:\n",
    "                term_ids[w] = curr_term_id\n",
    "                curr_term_id += 1\n",
    "    return (term_ids, nnz)\n",
    "\n",
    "def build_sparse_matrix(docs, term_ids = {}, nnz = 0):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    ncols = len(term_ids)\n",
    "    assert(ncols != 0)\n",
    "\n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows + 1, dtype=np.int)\n",
    "    row_id = 0  # document ID / row counter\n",
    "    acc = 0  # non-zero counter\n",
    "\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k, _ in cnt.most_common())\n",
    "        curr_doc_len = len(keys)\n",
    "        for i, key in enumerate(keys):\n",
    "            ind[acc + i] = term_ids[key]\n",
    "            val[acc + i] = cnt[key]\n",
    "        ptr[row_id + 1] = ptr[row_id] + curr_doc_len\n",
    "        acc += curr_doc_len\n",
    "        row_id += 1\n",
    "\n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build shared termID dictionary\n",
    "term_ids, nnz = calc_term_ids([*docs_train, *docs_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 10000\n",
    "SAMPLE_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat_train = build_sparse_matrix(docs_train, term_ids, nnz)\n",
    "mat_train = build_sparse_matrix(docs_train[:TRAIN_SIZE], term_ids, nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat_test = build_sparse_matrix(docs_test, term_ids, nnz)\n",
    "mat_test = build_sparse_matrix(docs_test[:SAMPLE_SIZE], term_ids, nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale matrix and normalize its rows\n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf. \n",
    "    Returns scaling factors as dict. If copy is True, \n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else mat\n",
    "\n",
    "def csr_l2normalize(mat, copy=False):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n",
    "    \n",
    "def csr_normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" csr_idf + csr_l2normalize\"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    csr_idf(mat)\n",
    "    csr_l2normalize(mat)\n",
    "    if copy is True:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_test = csr_normalize(mat_test, copy=True)\n",
    "mat_train = csr_normalize(mat_train, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "train_data = mat_train\n",
    "\n",
    "def predict(test_data, verbose=False):\n",
    "    nrows = test_data.shape[0]\n",
    "    predictions = []\n",
    "    for i, data in enumerate(test_data):\n",
    "        if verbose:\n",
    "            print( \"- Starting test data %s...\" % (i) )\n",
    "        predictions.append(knn(data, train_data, k=K, verbose=verbose))\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "# Cosine similarity\n",
    "from scipy.sparse.linalg import norm\n",
    "def dist(a, b):\n",
    "    dp = a.dot(b.T).todense().item()\n",
    "    return dp / ( norm(a) * norm(b))\n",
    "\n",
    "def knn(point, cluster, k, verbose=False):\n",
    "    dists = []\n",
    "    \n",
    "    # Calculate distance b/ the given node to all other nodes\n",
    "    for i, node in enumerate(cluster):\n",
    "        d = dist(point, node)\n",
    "        if verbose:\n",
    "            print( \"- Compared with train data %s. Dist: %s\" % (i, d) )\n",
    "        dists.append({\"dist\": d, \"label\": y_train[i]})\n",
    "    \n",
    "    # Sort and truncate by K\n",
    "    dists = sorted(dists, reverse=True, key = lambda i: i[\"dist\"])[:k]\n",
    "    if verbose:\n",
    "        print( \"- Dist truncated by K=%d:\" % (k), dists )\n",
    "\n",
    "    # Add weight for classification\n",
    "    weights = {}\n",
    "    for label, v in groupby(dists, key = lambda i: i[\"label\"]):\n",
    "        weights[label] = sum(item[\"dist\"] for item in list(v))\n",
    "    \n",
    "    return max(weights.items(), key=itemgetter(1))[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = predict(mat_test, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 4, 5, 1, 4, 4, 3, 5, 1, 1, 3, 4, 4, 5, 4, 3, 3, 5, 5, 5, 1, 5, 3, 5, 5, 5, 4, 4, 4, 4, 5, 1, 5, 4, 4, 3, 1, 4, 3, 3, 5, 1, 5, 5, 5, 5, 1, 1, 5, 5, 4, 1, 4, 5, 1, 5, 2, 5, 1, 2, 3, 2, 4, 3, 5, 1, 1, 4, 1, 5, 1, 4, 2, 5, 5, 5, 1, 5, 5, 5, 1, 3, 2, 5, 3, 1, 5, 1, 5, 1, 4, 1, 2, 4, 1, 1, 3, 4, 1]\n",
      "[1, 5, 3, 1, 5, 5, 2, 5, 4, 2, 1, 1, 1, 4, 4, 5, 2, 1, 1, 5, 5, 5, 4, 5, 3, 4, 4, 4, 5, 1, 5, 2, 5, 4, 4, 5, 1, 4, 2, 5, 5, 2, 4, 4, 1, 2, 1, 4, 5, 4, 1, 1, 3, 2, 3, 1, 4, 2, 1, 4, 1, 2, 1, 5, 3, 1, 3, 5, 3, 1, 4, 1, 1, 5, 5, 3, 4, 3, 1, 1, 5, 1, 1, 5, 4, 1, 5, 5, 5, 5, 4, 2, 4, 3, 3, 4, 4, 3, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "print(p)\n",
    "print(y_test[:SAMPLE_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16706947825559204"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test[:SAMPLE_SIZE], p, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "p = predict(mat_test, verbose=False)\n",
    "f1_score(y_test[:SAMPLE_SIZE], p, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4\n",
    "p = predict(mat_test, verbose=False)\n",
    "f1_score(y_test[:SAMPLE_SIZE], p, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "p = predict(mat_test, verbose=False)\n",
    "f1_score(y_test[:SAMPLE_SIZE], p, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6\n",
    "p = predict(mat_test, verbose=False)\n",
    "f1_score(y_test[:SAMPLE_SIZE], p, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 7\n",
    "p = predict(mat_test, verbose=False)\n",
    "f1_score(y_test[:SAMPLE_SIZE], p, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 8\n",
    "p = predict(mat_test, verbose=False)\n",
    "f1_score(y_test[:SAMPLE_SIZE], p, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 9\n",
    "p = predict(mat_test, verbose=False)\n",
    "f1_score(y_test[:SAMPLE_SIZE], p, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
